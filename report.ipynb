{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eBpsZoKRdegp",
        "Xcz-IXRPdhYC"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBpsZoKRdegp"
      },
      "source": [
        "# Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqc5oU261RNh"
      },
      "source": [
        "from math import sqrt, pi, exp\n",
        "from random import seed, randrange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import operator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcz-IXRPdhYC"
      },
      "source": [
        "# Reading and normalizing of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7jevaoX1WZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "a2bbe916-43a3-4351-8a99-7f774f6ad7a7"
      },
      "source": [
        "# Reading the dataset with pandas\n",
        "dataset = pd.read_csv('glass.csv')\n",
        "dataset2 = pd.read_csv('Concrete_Data_Yeh.csv')\n",
        "\n",
        "dataset = dataset.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e480e927ac29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading the dataset with pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glass.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Concrete_Data_Yeh.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glass.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-em1zGLwNfuG"
      },
      "source": [
        "# Min-max normalization\n",
        "normalized_dataset = np.copy(dataset)\n",
        "\n",
        "for i in range(9):\n",
        "    v = normalized_dataset[:, i]\n",
        "    normalized_dataset[:, i] = (v - v.min()) / (v.max() - v.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v7Ho6t3mg0a"
      },
      "source": [
        "# Min-max normalization\n",
        "normalized_dataset2 = dataset2\n",
        "\n",
        "for i in range(8):\n",
        "    v = normalized_dataset2.iloc[:, i]\n",
        "    normalized_dataset2.iloc[:, i] = (v - v.min()) / (v.max() - v.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56AlbENUobll"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HITN8wB4vmp"
      },
      "source": [
        "def euclidean_distance(row1, row2):\n",
        "    distance = 0.0\n",
        "    for i in range(len(row1) - 1):\n",
        "        distance += (row1[i] - row2[i])**2\n",
        "    return sqrt(distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz82GNVtxNcd"
      },
      "source": [
        "def make_weights(k, distances):\n",
        "    result = np.zeros(k, dtype=np.float32)\n",
        "    \n",
        "    for i in range(k):\n",
        "        if distances[i] == 0:\n",
        "            result[i] += 1 / 0.000000001\n",
        "        else:\n",
        "            result[i] += 1 / distances[i]\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As-ZEfa3vKrY"
      },
      "source": [
        "def get_weighted_pred(k, k_near_dists, data, ordering):\n",
        "    votes = dict()\n",
        "\n",
        "    wts = make_weights(k, k_near_dists)\n",
        "    \n",
        "    # wts = [20, 10, 8, 5, 4]\n",
        "    # distance = [10, 20, 25, 40, 55] - train[50], train[1], train[2]... len(train) = 164\n",
        "    # ordering = [50, 1, 2, 4, 3]\n",
        "    # type = [1, 2, 1, 3, 2] - train[50][-1], train[1][-1], train[2][-1]\n",
        "\n",
        "    for i in range(k):\n",
        "        idx = ordering[i]\n",
        "        pred_class = data[idx][-1]\n",
        "        if str(pred_class) not in votes.keys():\n",
        "            votes[str(pred_class)] = wts[i]\n",
        "        else:\n",
        "            votes[str(pred_class)] += wts[i]\n",
        "        \n",
        "    # finding the most predicted class in weighted predictions\n",
        "    max_k = -999999999\n",
        "    max_v = -999999999\n",
        "    for k, v in votes.items():\n",
        "        if v > max_v:\n",
        "            max_v = v\n",
        "            max_k = k\n",
        "    \n",
        "    return float(max_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPk-9dGWPc2j"
      },
      "source": [
        "# Locate the most similar neighbors\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "    distances = np.zeros(len(train))\n",
        "    for i in range(len(train)):\n",
        "        distances[i] = euclidean_distance(test_row, train[i])\n",
        "    \n",
        "    ordering = distances.argsort()\n",
        "\n",
        "    k_near_dists = np.zeros(num_neighbors)\n",
        "    neighbors = []\n",
        "    for i in range(num_neighbors):\n",
        "        idx = ordering[i]\n",
        "        k_near_dists[i] = distances[idx]  # save dists\n",
        "        neighbors.append(train[idx])\n",
        "    \n",
        "    return neighbors, get_weighted_pred(num_neighbors, k_near_dists, train, ordering)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9lgmGmjP943"
      },
      "source": [
        "# Make a classification prediction with neighbors\n",
        "def predict_classification(train, test_row, num_neighbors):\n",
        "\tneighbors, weighted_prediction = get_neighbors(train, test_row, num_neighbors)\n",
        "\toutput_values = [row[-1] for row in neighbors]\n",
        "\tprediction = max(output_values, key=output_values.count)\n",
        "\t\n",
        "\treturn prediction, weighted_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urn1j_0fTn5E"
      },
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    \n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold) \n",
        "\n",
        "    return dataset_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpIdBnaZd4mC"
      },
      "source": [
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\n",
        "\treturn correct / len(actual) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5eqm3M6QFs0"
      },
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "\n",
        "    scores = []\n",
        "    weighted_scores = []\n",
        "    for i, fold in enumerate(folds):\n",
        "        # splitting the test set\n",
        "        train_set = deepcopy(folds)\n",
        "        del train_set[i]\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = []\n",
        "        for row in fold:\n",
        "            row_copy = np.copy(row)\n",
        "            row_copy[-1] = None\n",
        "            test_set.append(row_copy)\n",
        "            \n",
        "\n",
        "        predicted, weighted_predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "        weighted_scores.append(accuracy_metric(actual, weighted_predicted))\n",
        "    \n",
        "\n",
        "    return scores, weighted_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eg2p5bMol6d"
      },
      "source": [
        "# kNN Algorithm\n",
        "def k_nearest_neighbors(train, test, num_neighbors):\n",
        "    predictions = list()\n",
        "    weighted_predictions = list()\n",
        "    for row in test:\n",
        "        output, weighted_prediction = predict_classification(train, row, num_neighbors)\n",
        "        predictions.append(output)\n",
        "        weighted_predictions.append(weighted_prediction)\n",
        "\n",
        "    return predictions, weighted_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc6Xzlcimch1"
      },
      "source": [
        "When the k value is increased, we can see that accuracy of the folds -and mean accuracy too - is decreased. Since there is not really much value to train, increasing the k value causes include the pointless (useless) points, and because of this, model can not predict the samples with a good accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRI5XWjmg0d"
      },
      "source": [
        "# REGRESSION AND WEIGHTED KNN/KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTsNt7r-mg0d"
      },
      "source": [
        "Gaussian function for weights of knn which takes distance and sigma value the user chooses.\n",
        "Sigma changes the gaussian graph which is affecting the output of the algorithm depending on the dataset given.\n",
        "With our dataset and weighted knn, the sigma value had to be higher, othervise it would give results close to zeros.\n",
        "Sigma values therefore is assigned as 13."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeQQHVU8mg0e"
      },
      "source": [
        "def gaussian(dist, sigma=13):   # weighted knn\n",
        "        return 1./(sqrt(2.*pi)*sigma)*exp(-dist**2/(2*sigma**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLdIAUrCmg0e"
      },
      "source": [
        "Predict function handles the knn part, which has the inputs of xtest xtrain ytrain k and boolean weighted to give information about if the given datas will be used as weighted knn or knn.\n",
        "Firstly, dataset is changed to pandas if numpy array and an empty predictions array is set.\n",
        "Secondly, the distances are found within the loop and appended to the distances array with ytrain row as well and sorted.\n",
        "Lastly, if the weighted is true, the function uses the gaussian weight with distances array inputed to its function and distances are multiplied with the weights and the predictions are added to array as divided by the total weight, else if the weighted is false, the distances are fed as values and the prediction is added as value divided by given k which is number of nearest neighbours given. Output is the prediction array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOXVLNiPmg0f"
      },
      "source": [
        "def predict(xTest, xTrain, yTrain, k, weighted):   #weighted knn/ normal knn\n",
        "        \n",
        "        if isinstance(xTest, np.ndarray):\n",
        "            xTest = pd.DataFrame(xTest)\n",
        "            \n",
        "        if isinstance(xTrain, np.ndarray):\n",
        "            xTrain = pd.DataFrame(xTrain)\n",
        "        \n",
        "        predictions = []\n",
        "        \n",
        "        for i in range(len(xTest)):\n",
        "            #newdist = np.zeros(len(yTrain))\n",
        "            \n",
        "            distances = []\n",
        "            \n",
        "            for j in range(len(yTrain)):\n",
        "                distanc = distance(xTrain.iloc[j,:], xTest.iloc[i,:])\n",
        "                distances.append((yTrain[j], distanc))\n",
        "                #newdist[j] = distance(xTrain.iloc[j,:], xTest.iloc[i,:])\n",
        "            \n",
        "            #newdist = np.array([newdist, yTrain])  # distances\n",
        "            #idx = np.argsort(newdist[0,:])\n",
        "            #newdist = newdist[:,idx]\n",
        "            distances.sort(key=operator.itemgetter(1))\n",
        "            \n",
        "            v = 0\n",
        "            total_weight = 0\n",
        "            for i in range(k):\n",
        "                weight = gaussian(distances[i][1])\n",
        "                #print(str(weight) + 'for this sample')\n",
        "                if weighted:\n",
        "                    v += distances[i][0]*weight\n",
        "                else:\n",
        "                    v += distances[i][0]\n",
        "                total_weight += weight\n",
        "            #print('toal weight' + str(total_weight))\n",
        "            #print('v' + str(v))\n",
        "            if weighted:\n",
        "                predictions.append(v/total_weight)\n",
        "            else:\n",
        "                predictions.append(v/k)\n",
        "            \n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA1ut6slmg0f"
      },
      "source": [
        "Distance function used in knn which returns the euclidean distance given the two arrays of vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLdaCg2Lmg0g"
      },
      "source": [
        "def distance(pa,pb):\n",
        "    distance = 0\n",
        "    for i in range(len(pa)):\n",
        "        distance = distance + np.square(pa[i] - pb[i])\n",
        "    return distance**0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3QsrW-umg0g"
      },
      "source": [
        "5 fold cross validation split is handled here.\n",
        "The function takes the given dataset and splits it to 5 folds and the folds are outputted as array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK0Nd1iFmg0h"
      },
      "source": [
        "def crossValSplit(dataset):\n",
        "       \n",
        "        split = []\n",
        "        copy = dataset\n",
        "        foldSize = int(copy.shape[0] / 5)\n",
        "        print(len(copy))\n",
        "        for i in range(5):\n",
        "            fold = []\n",
        "            while len(fold) < foldSize:\n",
        "                r = randrange(copy.shape[0])\n",
        "                index = copy.index[r]\n",
        "                fold.append(copy.loc[index].values.tolist())\n",
        "                copy = copy.drop(index)\n",
        "            split.append(np.asarray(fold))\n",
        "        return split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgnllSm2mg0h"
      },
      "source": [
        "Calculate mean absolute error(mae) with the given actual array and the predicted array that is given by predict function of the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nsP_cEPmg0h"
      },
      "source": [
        "# Calculate mean absolute error\n",
        "def MAE(actual, predicted):\n",
        "    mae = 0\n",
        "    print('len(actual)' + str(len(actual)))\n",
        "    print('len(predicted)' + str(len(predicted)))\n",
        "    for i in range(len(actual)):\n",
        "        mae += abs(actual[i]-predicted[i])\n",
        "    \n",
        "    mae /= len(actual)\n",
        "    return mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1b9jI5_mg0i"
      },
      "source": [
        "Main function that handles the knn regression.\n",
        "Takes the given dataset, number of folds, k value of knn, and boolean of weight to use weighted or not.\n",
        "Firstly, folds are handled with the crossvalsplit function given the dataset and an array of scores are set as empty.\n",
        "Secondly, the folds are splitted as xTrain yTrain XTest and yTest in the loop with folds array and the cv arrray. folds[i][:,0:8] corresponds to xTest, cv[:,8:8] corresponds as xTrain and cv[:,8] corresponds as yTrain and the values of 8 are coming from the dataset since it has 9 columns present.\n",
        "These arrays are sent to predict function with booleans given as weighted or not.\n",
        "Lastly, the output of predict is used in the mae function to see the error of the predicted values of arrays and the scores of mae are appended to scores array and given as result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RJmoljEmg0i"
      },
      "source": [
        "def kFCVEvaluate(dataset, n_folds, k, weighted):\n",
        "        '''\n",
        "        Description:\n",
        "            Driver function for k-Fold cross validation \n",
        "        '''\n",
        "        #knn = kNNClassifier()\n",
        "        \n",
        "        folds = crossValSplit(dataset)\n",
        "        \n",
        "        scores = []\n",
        "        \n",
        "        anan = True\n",
        "        for i in range(5):   #splitting the dataset to xtrain ytrain xtest ytest for folds\n",
        "            r = list(range(5))\n",
        "            r.pop(i)\n",
        "            \n",
        "            for j in r:\n",
        "                if j == r[0]:\n",
        "                    cv = folds[j]\n",
        "                else:\n",
        "                    cv = np.concatenate((cv, folds[j]), axis = 0) # until here\n",
        "           \n",
        "            if weighted:\n",
        "                predicted = predict(folds[i][:,0:8], cv[:,0:8], cv[:,8], k, True)     #find neighbours and the predicted values\n",
        "            else:\n",
        "                neighborsnpred = predict( folds[i][:,0:8], cv[:,0:8], cv[:,8], k, False)  #find neighbours and the predicted values\n",
        "            \n",
        "            if weighted:\n",
        "                acc = MAE(folds[i][:,8], predicted)    #finding the mae of each fold\n",
        "                scores.append(acc)\n",
        "            else:\n",
        "                acc = MAE(folds[i][:,8], neighborsnpred)   #finding the mae of each fold\n",
        "                scores.append(acc)\n",
        "            print('fold'  + str(i) + 'done, accuracy:' + str(acc) )\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcDu2Av8tJCE"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nm0HrevtFv4"
      },
      "source": [
        "seed(1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "\n",
        "flag = False\n",
        "for data in [dataset, normalized_dataset]:\n",
        "    print(\"##########################################################\")\n",
        "    if flag:\n",
        "        print(\"Dataset with feature normalization\")\n",
        "    else:\n",
        "        print(\"Dataset without feature normalization\")\n",
        "        flag = True\n",
        "\n",
        "    for num_neighbors in [1, 3, 5, 7, 9]:\n",
        "        scores, weighted_scores = evaluate_algorithm(data, k_nearest_neighbors, n_folds, num_neighbors)\n",
        "        print(\"----------------------------------------------------------\")\n",
        "        print(num_neighbors, \"neighbor(s)\")\n",
        "        print('Fold scores: %s' % scores)\n",
        "        print('Mean accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "        print('Fold (weighted) scores: %s' % weighted_scores)\n",
        "        print('Mean accuracy: %.3f%%' % (sum(weighted_scores) / float(len(weighted_scores))))\n",
        "\n",
        "\n",
        "#-------------- regresssion --------------\n",
        "for num_neighbors in [1, 3, 5, 7, 9]:\n",
        "    scores = kFCVEvaluate(normalized_dataset2, n_folds, num_neighbors, False) # knn + regression\n",
        "    print(scores)\n",
        "    print(num_neighbors, \"neighbor(s)\")\n",
        "    print('Mean Absolute Error over 5 folds: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "\n",
        "#-----------weighted regression-----------\n",
        "for num_neighbors in [1, 3, 5, 7, 9]:\n",
        "    scores2 = kFCVEvaluate(normalized_dataset2, n_folds, num_neighbors, True) # weighted knn + regression\n",
        "    print(scores2)\n",
        "    print(num_neighbors, \"neighbor(s)\")\n",
        "    print('Mean Absolute Error over 5 folds: %.3f%%' % (sum(scores2)/float(len(scores2))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZOHXk8xX7Av"
      },
      "source": [
        "||Fold 1|Fold 2|Fold 3|Fold 4|Fold 5|Fold avg|\n",
        "|------|------|------|------|------|------|--------|\n",
        "|1nn non-normalized classification |83.333  |57.142  | 64.285 | 76.190 | 80.952 | 72.381   |\n",
        "|3nn non-normalized classification|78.571  |80.952  | 66.666 | 66.666 | 64.285 | 71.429  |\n",
        "|5nn non-normalized classification|69.047  |78.571  | 69.047 | 64.285 | 59.523 | 68.095   |\n",
        "|7nn non-normalized classification|64.285  |57.142  | 64.285 | 69.047 | 69.047 | 64.762   |\n",
        "|9nn non-normalized classification|69.047  |54.761  | 52.380 | 64.285 | 66.666 | 61.429   |\n",
        "|1nn normalized classification |61.904  |78.571  | 50.0 | 69.047 | 78.571 | 67.619   |\n",
        "|3nn normalized classification|76.190  |59.523  | 78.571 | 57.142 | 61.904 | 66.667   |\n",
        "|5nn normalized classification|61.904  |61.904  | 64.285 | 73.809 | 64.285 | 65.238   |\n",
        "|7nn normalized classification|69.047  |57.142  | 80.952 | 61.904 | 73.809 | 68.571   |\n",
        "|9nn normalized classification|69.047  |61.904  | 52.380 | 73.809 | 69.047 | 65.238   |\n",
        "|w-1nn non-normalized classification |83.333  |57.142  | 64.285 | 76.190 | 80.952 | 72.381|\n",
        "|w-3nn non-normalized classification|78.571  |80.952  | 69.047 | 66.666 | 66.666 | 72.381  |\n",
        "|w-5nn non-normalized classification|69.047  |78.571  | 69.047 | 66.666 | 61.904 | 69.048  |\n",
        "|w-7nn non-normalized classification|64.285  |64.285  | 64.285 | 66.666 | 71.428 | 66.190  |\n",
        "|w-9nn non-normalized classification|73.809  |59.523  | 59.523 | 71.428 | 69.047 | 66.667  |\n",
        "|w-1nn normalized classification |61.904  |78.571  | 50.0 | 69.047 | 78.571 | 67.619|\n",
        "|w-3nn normalized classification|76.190  |59.523  | 78.571 | 57.142 | 61.904 | 66.667  |\n",
        "|w-5nn normalized classification|66.666  |64.285  | 66.666 | 73.809 | 66.666 | 67.619  |\n",
        "|w-7nn normalized classification|73.809  |59.523  | 80.952 | 66.666 | 71.428 | 70.476  |\n",
        "|w-9nn normalized classification|71.428  |66.666  | 59.523 | 78.571 | 71.428 | 69.524  |\n",
        "|1nn non-normalized  regression|6.189  |6.334  | 7.249 | 6.506 | 6.467 | 6.549   |\n",
        "|3nn non-normalized regression|6.117  |6.330  | 7.005 | 6.574 | 6.850 | 6.576   |\n",
        "|5nn non-normalized regression|5.824  |7.213  | 7.348 | 7.285 | 6.9308 | 6.921   |\n",
        "|7nn non-normalized regression|6.055  |7.710  | 7.455 | 7.490 | 7.404 | 7.223   |\n",
        "|9nn non-normalized regression|6.418  |7.719  | 7.514 | 7.827 | 7.739 | 7.444   |\n",
        "|1nn normalized  regression|6.189  |6.334  | 7.249 | 6.506 | 6.467 | 6.549   |\n",
        "|3nn normalized regression|6.117  |6.330  | 7.005 | 6.5741 | 6.850 | 6.576   |\n",
        "|5nn normalized regression|5.824  |7.213  | 7.348 | 7.285 | 6.930 | 6.921   |\n",
        "|7nn normalized regression|6.055  |7.710  | 7.455 | 7.490 | 7.404 | 7.223   |\n",
        "|9nn normalized regression|6.418  |7.719  | 7.514 | 7.827 | 7.739 | 7.444   |\n",
        "|w-1nn non-normalized  regression|6.534  |5.367  | 7.097 | 6.224 | 5.956 | 6.236   |\n",
        "|w-3nn non-normalized regression|6.827  |5.769  | 6.058 | 5.981 | 5.773 | 6.082   |\n",
        "|w-5nn non-normalized regression|5.751  |5.589  | 6.735 | 5.601 | 5.664 | 5.869   |\n",
        "|w-7nn non-normalized regression|5.772  |5.719  | 6.736 | 5.657 | 5.702 | 5.918   |\n",
        "|w-9nn non-normalized regression|5.759  |5.754  | 6.700 | 5.680 | 5.693 | 5.918   |\n",
        "|w-1nn normalized  regression|6.189  |6.334  | 7.249 | 6.506 | 6.467 | 6.549   |\n",
        "|w-3nn normalized regression|6.117  |6.330  | 7.005 | 6.574 | 6.850 | 6.576   |\n",
        "|w-5nn normalized regression|5.824  |7.213  | 7.348 | 7.285 | 6.930 | 6.921   |\n",
        "|w-7nn normalized regression|6.055  |7.710  | 7.455 | 7.490 | 7.404 | 7.223   |\n",
        "|w-9nn normalized regression|6.418  |7.719  | 7.514 | 7.827 | 7.739 | 7.444   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLP-vZx1mg0k"
      },
      "source": [
        "Followings are the results of the regression part with normalized/not normalized datasets, weighted/normal knn used and the k of nn used from 1 to 9.\n",
        "\n",
        "Firstly, i have noticed the sigma values importance in the weighted knn since the results were NaN if it was too low. This makes me understand that the gaussian was too sharply increasing which ended up giving multiply/divide by zero result and created NaN output. I have played with the sigma values from 1 to 33 to see the difference and used 13 lastly which are used in the tested data results as well.\n",
        "\n",
        "Secondly, i have seen the Mean absolute error was incresing with normalized dataset which may be related to overfitting or not enough information is taken or there may be residual noise. This shows that the data must be invested more deeply to see if anything can be done.\n",
        "\n",
        "Lastly, the nearest neighbour values given the program increased the accuracy of the not normalized folds of the dataset's results, but the opposite has occured with the normalized folds of the dataset. This shows the effect of the weights and the nearest neighbours given the dataset. For this part, since the accuracy of the smaller k was higher, we can say that k value must be lower for normalized weighted knn for regression and higher for the not normalized weighted knn regression.\n",
        "\n",
        "For the knn of the not weighted regression part, the normalization did not have an effect on the results for our tests. This might be a sign that the weighted regressions change of normalized dataset might be due to the weights change. Other than that, the results shows that the lower k value for knn works better with the programs results, so the knn's k must be lower.\n",
        "\n",
        "Test set length is 206 as can be seen from the followings.\n",
        "Test set length for classification is 42."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSyRArGDmg0k"
      },
      "source": [
        "1NN-REGRESS: [6.18907766990291, 6.334223300970872, 7.24941747572815, 6.506747572815539, 6.467135922330095]\n",
        "Mean Absolute Error over 5 folds: 6.549%  len(actual)206\n",
        "len(predicted)206\n",
        "3NN-REGRESS: [6.11742718446602, 6.330922330097083, 7.005388349514564, 6.5741100323624595, 6.850792880258901]\n",
        "Mean Absolute Error over 5 folds: 6.576%\n",
        "5NN-REGRESS:  [5.824592233009711, 7.213553398058253, 7.348611650485441, 7.2851747572815535, 6.930873786407765]\n",
        "Mean Absolute Error over 5 folds: 6.921%\n",
        "7NN-REGRESS: [6.055048543689317, 7.7104854368932, 7.455755894590844, 7.490277392510404, 7.404861303744797]\n",
        "Mean Absolute Error over 5 folds: 7.223%\n",
        "9NN-REGRESS:  [6.418322545846825, 7.719471413160735, 7.51416396979504, 7.827713052858686, 7.739611650485441]\n",
        "Mean Absolute Error over 5 folds: 7.444%\n",
        "\n",
        "1NN-REGRESS-NORMALIZED: [6.18907766990291, 6.334223300970872, 7.24941747572815, 6.506747572815539, 6.467135922330095]\n",
        "Mean Absolute Error over 5 folds: 6.549%\n",
        "3NN-REGRESS-NORMALIZED: [6.11742718446602, 6.330922330097083, 7.005388349514564, 6.5741100323624595, 6.850792880258901]\n",
        "Mean Absolute Error over 5 folds: 6.576%\n",
        "5NN-REGRESS-NORMALIZED: [5.824592233009711, 7.213553398058253, 7.348611650485441, 7.2851747572815535, 6.930873786407765]\n",
        "Mean Absolute Error over 5 folds: 6.921%\n",
        "7NN-REGRESS-NORMALIZED: [6.055048543689317, 7.7104854368932, 7.455755894590844, 7.490277392510404, 7.404861303744797]\n",
        "Mean Absolute Error over 5 folds: 7.223%\n",
        "9NN-REGRESS-NORMALIZED: [6.418322545846825, 7.719471413160735, 7.51416396979504, 7.827713052858686, 7.739611650485441]\n",
        "Mean Absolute Error over 5 folds: 7.444%\n",
        "\n",
        "W-1NN-REGRESS-SIGMA13: [6.534223300970872, 5.367718446601936, 7.097378640776697, 6.224320388349516, 5.956310679611649]\n",
        "Mean Absolute Error over 5 folds: 6.236%  TESTLENlen(actual)206len(predicted)206\n",
        "W-3NN-REGRESS-SIGMA13: [6.827666203320355, 5.769247399492046, 6.058359499635622, 5.981974632804876, 5.773720404917418]\n",
        "Mean Absolute Error over 5 folds: 6.082%  TESTLENlen(actual)206len(predicted)206\n",
        "W-5NN-REGRESS-SIGMA13: [5.751891957484146, 5.589408043621229, 6.735197398163901, 5.6018279820702706, 5.66446546796987]\n",
        "Mean Absolute Error over 5 folds: 5.869%  TESTLENlen(actual)206len(predicted)206\n",
        "W-7NN-REGRESS-SIGMA13: [5.772153735840744, 5.719477453333139, 6.73689808643413, 5.657603921706346, 5.702888511421789]\n",
        "Mean Absolute Error over 5 folds: 5.918%  TESTLENlen(actual)206len(predicted)206\n",
        "W-9NN-REGRESS-SIGMA13: [5.759090328991352, 5.754029501069371, 6.700971001729926, 5.68084465515428, 5.693954077208931]\n",
        "Mean Absolute Error over 5 folds: 5.918%\n",
        "\n",
        "W-1NN-REGRESS-SIGMA13-NORMALIZED: [6.18907766990291, 6.334223300970872, 7.24941747572815, 6.506747572815537, 6.467135922330095]\n",
        "Mean Absolute Error over 5 folds: 6.549%  TESTLENlen(actual)206len(predicted)206\n",
        "W-3NN-REGRESS-SIGMA13-NORMALIZED: [6.117400215376787, 6.330900400859135, 7.005365428185682, 6.574047101539171, 6.850742843943034]\n",
        "Mean Absolute Error over 5 folds: 6.576%  TESTLENlen(actual)206len(predicted)206\n",
        "W-5NN-REGRESS-SIGMA13-NORMALIZED: [5.824583538628261, 7.213478652955402, 7.348613554367653, 7.28506761641721, 6.930854265889408]\n",
        "Mean Absolute Error over 5 folds: 6.921%  TESTLENlen(actual)206len(predicted)206\n",
        "W-7NN-REGRESS-SIGMA13-NORMALIZED: [6.055019074556912, 7.710397711022147, 7.455749072424086, 7.490199244768782, 7.404802044829571]\n",
        "Mean Absolute Error over 5 folds: 7.223%  TESTLENlen(actual)206len(predicted)206\n",
        "W-9NN-REGRESS-SIGMA13-NORMALIZED: [6.418255722902209, 7.719439535702919, 7.514160622915275, 7.827615862623473, 7.739534914184686]\n",
        "Mean Absolute Error over 5 folds: 7.444%  TESTLENlen(actual)206len(predicted)206"
      ]
    }
  ]
}