# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uiM6NGfRRC6VsgAnD5UR3KOBIri3_MiE

# Importing libraries
"""

from math import sqrt, pi, exp
from random import seed, randrange
import pandas as pd
import numpy as np
from copy import deepcopy
import operator

"""# Reading and normalizing of the dataset"""

# Reading the dataset with pandas
dataset = pd.read_csv('glass.csv')
dataset2 = pd.read_csv('Concrete_Data_Yeh.csv')

dataset = dataset.values

# Min-max normalization
normalized_dataset = np.copy(dataset)

for i in range(9):
    v = normalized_dataset[:, i]
    normalized_dataset[:, i] = (v - v.min()) / (v.max() - v.min())

# Min-max normalization
normalized_dataset2 = dataset2

for i in range(8):
    v = normalized_dataset2.iloc[:, i]
    normalized_dataset2.iloc[:, i] = (v - v.min()) / (v.max() - v.min())


"""# Classification"""


def euclidean_distance(row1, row2):
    distance = 0.0
    for i in range(len(row1) - 1):
        distance += (row1[i] - row2[i])**2
    return sqrt(distance)


def make_weights(k, distances):
    result = np.zeros(k, dtype=np.float32)

    for i in range(k):
        if distances[i] == 0:
            result[i] += 1 / 0.000000001
        else:
            result[i] += 1 / distances[i]

    return result


def get_weighted_pred(k, k_near_dists, data, ordering):
    votes = dict()

    wts = make_weights(k, k_near_dists)

    # wts = [20, 10, 8, 5, 4]
    # distance = [10, 20, 25, 40, 55] - train[50], train[1], train[2]... len(train) = 164
    # ordering = [50, 1, 2, 4, 3]
    # type = [1, 2, 1, 3, 2] - train[50][-1], train[1][-1], train[2][-1]

    for i in range(k):
        idx = ordering[i]
        pred_class = data[idx][-1]
        if str(pred_class) not in votes.keys():
            votes[str(pred_class)] = wts[i]
        else:
            votes[str(pred_class)] += wts[i]

    # finding the most predicted class in weighted predictions
    max_k = -999999999
    max_v = -999999999
    for k, v in votes.items():
        if v > max_v:
            max_v = v
            max_k = k

    return float(max_k)


# Locate the most similar neighbors
def get_neighbors(train, test_row, num_neighbors):
    distances = np.zeros(len(train))
    for i in range(len(train)):
        distances[i] = euclidean_distance(test_row, train[i])

    ordering = distances.argsort()

    k_near_dists = np.zeros(num_neighbors)
    neighbors = []
    for i in range(num_neighbors):
        idx = ordering[i]
        k_near_dists[i] = distances[idx]  # save dists
        neighbors.append(train[idx])

    return neighbors, get_weighted_pred(num_neighbors, k_near_dists, train, ordering)


# Make a classification prediction with neighbors
def predict_classification(train, test_row, num_neighbors):
    neighbors, weighted_prediction = get_neighbors(train, test_row, num_neighbors)
    output_values = [row[-1] for row in neighbors]
    prediction = max(output_values, key=output_values.count)

    return prediction, weighted_prediction


# Split a dataset into k folds
def cross_validation_split(dataset, n_folds):
    dataset_split = list()
    dataset_copy = list(dataset)

    fold_size = int(len(dataset) / n_folds)
    for _ in range(n_folds):
        fold = list()
        while len(fold) < fold_size:
            index = randrange(len(dataset_copy))
            fold.append(dataset_copy.pop(index))
        dataset_split.append(fold)

    return dataset_split


# Calculate accuracy percentage
def accuracy_metric(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1

    return correct / len(actual) * 100


# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
    folds = cross_validation_split(dataset, n_folds)

    scores = []
    weighted_scores = []
    for i, fold in enumerate(folds):
        # splitting the test set
        train_set = deepcopy(folds)
        del train_set[i]
        train_set = sum(train_set, [])
        test_set = []
        for row in fold:
            row_copy = np.copy(row)
            row_copy[-1] = None
            test_set.append(row_copy)

        predicted, weighted_predicted = algorithm(train_set, test_set, *args)
        actual = [row[-1] for row in fold]
        accuracy = accuracy_metric(actual, predicted)
        scores.append(accuracy)
        weighted_scores.append(accuracy_metric(actual, weighted_predicted))

    return scores, weighted_scores


# kNN Algorithm
def k_nearest_neighbors(train, test, num_neighbors):
    predictions = list()
    weighted_predictions = list()
    for row in test:
        output, weighted_prediction = predict_classification(train, row, num_neighbors)
        predictions.append(output)
        weighted_predictions.append(weighted_prediction)

    return predictions, weighted_predictions


"""When the k value is increased, we can see that accuracy of the folds -and mean accuracy too - is decreased. Since there is not really much value to train, increasing the k value causes include the pointless (useless) points, and because of this, model can not predict the samples with a good accuracy.

# REGRESSION AND WEIGHTED KNN/KNN

Gaussian function for weights of knn which takes distance and sigma value the user chooses.
Sigma changes the gaussian graph which is affecting the output of the algorithm depending on the dataset given.
With our dataset and weighted knn, the sigma value had to be higher, othervise it would give results close to zeros.
Sigma values therefore is assigned as 13.
"""


def gaussian(dist, sigma=13):   # weighted knn
    return 1./(sqrt(2.*pi)*sigma)*exp(-dist**2/(2*sigma**2))


"""Predict function handles the knn part, which has the inputs of xtest xtrain ytrain k and boolean weighted to give information about if the given datas will be used as weighted knn or knn.
Firstly, dataset is changed to pandas if numpy array and an empty predictions array is set.
Secondly, the distances are found within the loop and appended to the distances array with ytrain row as well and sorted.
Lastly, if the weighted is true, the function uses the gaussian weight with distances array inputed to its function and distances are multiplied with the weights and the predictions are added to array as divided by the total weight, else if the weighted is false, the distances are fed as values and the prediction is added as value divided by given k which is number of nearest neighbours given. Output is the prediction array.
"""


def predict(xTest, xTrain, yTrain, k, weighted):   #weighted knn/ normal knn

    if isinstance(xTest, np.ndarray):
        xTest = pd.DataFrame(xTest)

    if isinstance(xTrain, np.ndarray):
        xTrain = pd.DataFrame(xTrain)

    predictions = []

    for i in range(len(xTest)):
        # newdist = np.zeros(len(yTrain))

        distances = []

        for j in range(len(yTrain)):
            distanc = distance(xTrain.iloc[j,:], xTest.iloc[i,:])
            distances.append((yTrain[j], distanc))
            # newdist[j] = distance(xTrain.iloc[j,:], xTest.iloc[i,:])

        # newdist = np.array([newdist, yTrain])  # distances
        # idx = np.argsort(newdist[0,:])
        # newdist = newdist[:,idx]
        distances.sort(key=operator.itemgetter(1))

        v = 0
        total_weight = 0
        for i in range(k):
            weight = gaussian(distances[i][1])
            #print(str(weight) + 'for this sample')
            if weighted:
                v += distances[i][0]*weight
            else:
                v += distances[i][0]
            total_weight += weight
        # print('toal weight' + str(total_weight))
        # print('v' + str(v))
        if weighted:
            predictions.append(v/total_weight)
        else:
            predictions.append(v/k)

    return predictions


"""Distance function used in knn which returns the euclidean distance given the two arrays of vectors."""


def distance(pa,pb):
    distance = 0
    for i in range(len(pa)):
        distance = distance + np.square(pa[i] - pb[i])
    return distance**0.5

"""5 fold cross validation split is handled here.
The function takes the given dataset and splits it to 5 folds and the folds are outputted as array.
"""


def crossValSplit(dataset):
    split = []
    copy = dataset
    foldSize = int(copy.shape[0] / 5)
    print(len(copy))
    for i in range(5):
        fold = []
        while len(fold) < foldSize:
            r = randrange(copy.shape[0])
            index = copy.index[r]
            fold.append(copy.loc[index].values.tolist())
            copy = copy.drop(index)
        split.append(np.asarray(fold))
    return split


"""Calculate mean absolute error(mae) with the given actual array and the predicted array that is given by predict function of the folds."""


# Calculate mean absolute error
def MAE(actual, predicted):
    mae = 0
    print('len(actual)' + str(len(actual)))
    print('len(predicted)' + str(len(predicted)))
    for i in range(len(actual)):
        mae += abs(actual[i]-predicted[i])

    mae /= len(actual)
    return mae


"""Main function that handles the knn regression.
Takes the given dataset, number of folds, k value of knn, and boolean of weight to use weighted or not.
Firstly, folds are handled with the crossvalsplit function given the dataset and an array of scores are set as empty.
Secondly, the folds are splitted as xTrain yTrain XTest and yTest in the loop with folds array and the cv arrray. folds[i][:,0:8] corresponds to xTest, cv[:,8:8] corresponds as xTrain and cv[:,8] corresponds as yTrain and the values of 8 are coming from the dataset since it has 9 columns present.
These arrays are sent to predict function with booleans given as weighted or not.
Lastly, the output of predict is used in the mae function to see the error of the predicted values of arrays and the scores of mae are appended to scores array and given as result.
"""


def kFCVEvaluate(dataset, n_folds, k, weighted):
    '''
    Description:
        Driver function for k-Fold cross validation
    '''
    # knn = kNNClassifier()

    folds = crossValSplit(dataset)

    scores = []

    for i in range(5):   # splitting the dataset to xtrain ytrain xtest ytest for folds
        r = list(range(5))
        r.pop(i)

        for j in r:
            if j == r[0]:
                cv = folds[j]
            else:
                cv = np.concatenate((cv, folds[j]), axis = 0) # until here

        if weighted:
            predicted = predict(folds[i][:,0:8], cv[:,0:8], cv[:,8], k, True)     # find neighbours and the predicted values
        else:
            neighborsnpred = predict( folds[i][:,0:8], cv[:,0:8], cv[:,8], k, False)  # find neighbours and the predicted values

        if weighted:
            acc = MAE(folds[i][:,8], predicted)    # finding the mae of each fold
            scores.append(acc)
        else:
            acc = MAE(folds[i][:,8], neighborsnpred)   # finding the mae of each fold
            scores.append(acc)
        print('fold'  + str(i) + 'done, accuracy:' + str(acc) )
    return scores


"""# Main"""

seed(1)
# evaluate algorithm
n_folds = 5

flag = False
for data in [dataset, normalized_dataset]:
    print("##########################################################")
    if flag:
        print("Dataset with feature normalization")
    else:
        print("Dataset without feature normalization")
        flag = True

    for num_neighbors in [1, 3, 5, 7, 9]:
        scores, weighted_scores = evaluate_algorithm(data, k_nearest_neighbors, n_folds, num_neighbors)
        print("----------------------------------------------------------")
        print(num_neighbors, "neighbor(s)")
        print('Fold scores: %s' % scores)
        print('Mean accuracy: %.3f%%' % (sum(scores) / float(len(scores))))
        print('Fold (weighted) scores: %s' % weighted_scores)
        print('Mean accuracy: %.3f%%' % (sum(weighted_scores) / float(len(weighted_scores))))


# -------------- regresssion --------------
for num_neighbors in [1, 3, 5, 7, 9]:
    scores = kFCVEvaluate(normalized_dataset2, n_folds, num_neighbors, False) # knn + regression
    print(scores)
    print(num_neighbors, "neighbor(s)")
    print('Mean Absolute Error over 5 folds: %.3f%%' % (sum(scores)/float(len(scores))))

# -----------weighted regression-----------
for num_neighbors in [1, 3, 5, 7, 9]:
    scores2 = kFCVEvaluate(normalized_dataset2, n_folds, num_neighbors, True) # weighted knn + regression
    print(scores2)
    print(num_neighbors, "neighbor(s)")
    print('Mean Absolute Error over 5 folds: %.3f%%' % (sum(scores2)/float(len(scores2))))
